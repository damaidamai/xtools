"""
é«˜æ•ˆçš„HTTPå­åŸŸåæšä¸¾å™¨ - ä¼˜åŒ–ç‰ˆ

ä¸»è¦æ”¹è¿›:
1. ä¿®å¤ ClientSession ä½œç”¨åŸŸé—®é¢˜
2. å¹¶è¡Œè¯·æ±‚ç­–ç•¥æå‡é€Ÿåº¦
3. æ‰¹é‡æ•°æ®åº“å†™å…¥å‡å°‘ I/O
4. å¯é€‰çš„ DNS é¢„æ£€æŸ¥
5. è‡ªé€‚åº”å¹¶å‘æ§åˆ¶
"""

from __future__ import annotations

import asyncio
import datetime as dt
import json
import os
import ssl
from typing import Dict, Optional, Set, Tuple, List
import aiohttp
from aiohttp import ClientTimeout, TCPConnector
from urllib.parse import urlparse

from loguru import logger
from sqlalchemy.exc import IntegrityError
from sqlmodel import select
from sqlmodel.ext.asyncio.session import AsyncSession

from .models import Subdomain, SubdomainRun, Wordlist
from .run_progress import clear_progress, clear_stop, increment_progress, is_stopped, set_progress

LOG_LIMIT = 4000
DEFAULT_WORDLIST_TYPE = "subdomain"

# é…ç½®å‚æ•°
MAX_CONCURRENT_REQUESTS = int(os.getenv("MAX_CONCURRENT_REQUESTS", "50"))
REQUEST_TIMEOUT = int(os.getenv("REQUEST_TIMEOUT", "5"))
MAX_RESPONSE_SIZE = int(os.getenv("MAX_RESPONSE_SIZE", "4096"))
VERIFY_SSL = os.getenv("VERIFY_SSL", "false").lower() == "true"
ENABLE_GET_FALLBACK = os.getenv("ENABLE_GET_FALLBACK", "true").lower() == "true"
USER_AGENT = os.getenv("USER_AGENT", "XTools/1.0 (HTTP Subdomain Enumerator)")

# DNS é…ç½®
DNS_TIMEOUT = int(os.getenv("DNS_TIMEOUT", "2"))
DNS_RETRIES = int(os.getenv("DNS_RETRIES", "2"))

_ssl_context: Optional[ssl.SSLContext] = None


def _append_log(log: str, new_line: str) -> str:
    combined = (log + "\n" + new_line).strip()
    if len(combined) > LOG_LIMIT:
        return combined[-LOG_LIMIT:]
    return combined


def _safe_snippet(text: str, limit: int = 200) -> str:
    """æ¸…ç†æ¢è¡Œå’Œå¤šä½™ç©ºç™½ï¼Œæˆªæ–­ä»¥é¿å…æ—¥å¿—è¿‡é•¿ã€‚"""
    return " ".join(text.split())[:limit]


def _get_ssl_context() -> ssl.SSLContext:
    """ç¼“å­˜ SSL é…ç½®ï¼Œé¿å…é‡å¤åˆ›å»º"""
    global _ssl_context
    if _ssl_context is None:
        ctx = ssl.create_default_context()
        if not VERIFY_SSL:
            ctx.check_hostname = False
            ctx.verify_mode = ssl.CERT_NONE
        _ssl_context = ctx
    return _ssl_context


async def _dns_resolve(subdomain: str, resolver) -> Tuple[bool, Optional[List[str]]]:
    """
    DNS è§£ææ£€æŸ¥ï¼Œè¿”å›æ˜¯å¦å­˜åœ¨å’Œ IP åˆ—è¡¨
    
    éœ€è¦å®‰è£…: pip install aiodns
    è¿”å›: (æ˜¯å¦å­˜åœ¨, IPåˆ—è¡¨)
    """
    if resolver is None:
        # æ²¡æœ‰ aiodns æ—¶ä½¿ç”¨å†…ç½®è§£æä½œä¸ºå…œåº•ï¼ŒåŒæ—¶å°è¯•è·å– IP
        try:
            loop = asyncio.get_running_loop()
            addrs = await asyncio.wait_for(
                loop.getaddrinfo(subdomain, None, proto=0, type=0, family=0),
                timeout=2,
            )
            ips = list({addr[4][0] for addr in addrs if addr[4]})
            return (True, ips) if ips else (True, None)
        except Exception as e:
            logger.debug("Fallback DNS resolve failed for {}: {}", subdomain, e)
            return False, None
    
    try:
        ips = []
        
        # å°è¯•æŸ¥è¯¢ A è®°å½• (IPv4)
        try:
            result = await asyncio.wait_for(resolver.query(subdomain, 'A'), timeout=2)
            ips.extend([r.host for r in result])
        except:
            pass
        
        # å°è¯•æŸ¥è¯¢ AAAA è®°å½• (IPv6)
        try:
            result = await asyncio.wait_for(resolver.query(subdomain, 'AAAA'), timeout=2)
            ips.extend([r.host for r in result])
        except:
            pass
        
        # å¦‚æœæœ‰ä»»ä½• IPï¼Œè¯´æ˜ DNS è®°å½•å­˜åœ¨
        if ips:
            return True, ips
        
        return False, None
        
    except Exception as e:
        logger.debug("DNS resolve failed for {}: {}", subdomain, e)
        # DNS æŸ¥è¯¢å¤±è´¥ï¼ŒæŒ‰ä¸å­˜åœ¨å¤„ç†ï¼Œé¿å…ç»§ç»­å‘èµ· HTTP è¯·æ±‚
        return False, None


def _extract_peer_ip(response: aiohttp.ClientResponse) -> Optional[str]:
    """ä»å“åº”è¿æ¥ä¸­æå–å¯¹ç«¯ IPï¼Œä½œä¸º DNS ç»“æœçš„è¡¥å……ã€‚"""
    try:
        if response.connection and response.connection.transport:
            peer = response.connection.transport.get_extra_info("peername")
            if peer and isinstance(peer, (tuple, list)) and len(peer) > 0:
                return peer[0]
    except Exception as e:
        logger.debug("Extract peer IP failed: {}", e)
    return None


async def _verify_subdomain_http(
    subdomain: str,
    session: aiohttp.ClientSession,
) -> Tuple[bool, Dict]:
    """
    å¹¶è¡ŒéªŒè¯å­åŸŸåHTTPæœåŠ¡
    
    ç­–ç•¥ï¼šå¹¶è¡Œå°è¯• HTTPS/HTTP HEADï¼Œå¤±è´¥åå°è¯• OPTIONS å’Œ GET
    """
    details = {
        'subdomain': subdomain,
        'method': None,
        'scheme': None,
        'status_code': None,
        'content_type': None,
        'content_length': None,
        'server': None,
        'title': None,
        'error': None,
        'response_time': None
    }

    try:
        start_time = dt.datetime.now()
        
        # å¹¶è¡Œç­–ç•¥ï¼šåŒæ—¶å°è¯• HTTPS å’Œ HTTP çš„ HEAD è¯·æ±‚
        tasks = [
            _try_request(session, 'HEAD', f"https://{subdomain}", details.copy(), start_time),
            _try_request(session, 'HEAD', f"http://{subdomain}", details.copy(), start_time),
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿”å›ç¬¬ä¸€ä¸ªæˆåŠŸçš„ç»“æœ
        for result in results:
            if not isinstance(result, Exception) and result[0]:
                result[1].setdefault('detected_by', result[1].get('method'))
                return result

        # å¦‚æœ HEAD éƒ½å¤±è´¥ï¼Œå°è¯• OPTIONS
        tasks = [
            _try_request(session, 'OPTIONS', f"https://{subdomain}", details.copy(), start_time),
            _try_request(session, 'OPTIONS', f"http://{subdomain}", details.copy(), start_time),
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if not isinstance(result, Exception) and result[0]:
                result[1].setdefault('detected_by', result[1].get('method'))
                return result

        # æœ€åå°è¯•å—é™çš„ GET
        if ENABLE_GET_FALLBACK:
            tasks = [
                _try_limited_get(session, f"https://{subdomain}", details.copy(), start_time),
                _try_limited_get(session, f"http://{subdomain}", details.copy(), start_time),
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if not isinstance(result, Exception) and result[0]:
                    result[1].setdefault('detected_by', result[1].get('method'))
                    return result

        return False, details

    except Exception as e:
        details['error'] = str(e)
        return False, details


async def _try_request(
    session: aiohttp.ClientSession,
    method: str,
    url: str,
    details: Dict,
    start_time: dt.datetime
) -> Tuple[bool, Dict]:
    """å°è¯•å•ä¸ªHTTPè¯·æ±‚"""
    try:
        async with session.request(
            method=method,
            url=url,
            allow_redirects=False,
            timeout=ClientTimeout(total=3)
        ) as response:
            details.update({
                'method': method,
                'scheme': urlparse(url).scheme,
                'status_code': response.status,
                'content_type': response.headers.get('content-type', ''),
                'content_length': response.headers.get('content-length', ''),
                'server': response.headers.get('server', ''),
                'response_time': (dt.datetime.now() - start_time).total_seconds()
            })

            if _is_valid_response(response.status):
                details.setdefault('detected_by', details.get('method'))
                peer_ip = _extract_peer_ip(response)
                if peer_ip:
                    details.setdefault('ip', peer_ip)
                    details.setdefault('ips', [])
                    if peer_ip not in details['ips']:
                        details['ips'].append(peer_ip)
                return True, details

            return False, details

    except asyncio.TimeoutError:
        details['error'] = 'timeout'
    except aiohttp.ClientConnectorError:
        details['error'] = 'connection_refused'
    except aiohttp.ClientError as e:
        details['error'] = f'http_error: {type(e).__name__}'
    except Exception as e:
        details['error'] = f'unknown_error: {type(e).__name__}'

    return False, details


async def _try_limited_get(
    session: aiohttp.ClientSession,
    url: str,
    details: Dict,
    start_time: dt.datetime
) -> Tuple[bool, Dict]:
    """æœ‰é™åˆ¶çš„GETè¯·æ±‚ï¼Œåªä¸‹è½½å°‘é‡æ•°æ®ï¼›ç¼ºå¤±æ ‡é¢˜æ—¶ä¼šå°è¯•ä¸€æ¬¡æ—  Range çš„å…œåº• GETã€‚"""
    try:
        headers = {'Range': f'bytes=0-{MAX_RESPONSE_SIZE-1}'}

        async with session.get(
            url=url,
            headers=headers,
            # è·Ÿéšè·³è½¬ä»¥è·å–æœ€ç»ˆé¡µé¢æ ‡é¢˜ï¼ˆå¸¸è§ http->https æˆ– www é‡å®šå‘ï¼‰
            allow_redirects=True,
            timeout=ClientTimeout(total=3)
        ) as response:

            details.update({
                'method': 'GET(limited)',
                'scheme': urlparse(str(response.url)).scheme,
                'status_code': response.status,
                'content_type': response.headers.get('content-type', ''),
                'content_length': response.headers.get('content-length', ''),
                'server': response.headers.get('server', ''),
                'response_time': (dt.datetime.now() - start_time).total_seconds(),
                'final_url': str(response.url),
                'redirected': bool(response.history),
            })

            if _is_valid_response(response.status):
                details.setdefault('detected_by', details.get('method'))
                peer_ip = _extract_peer_ip(response)
                if peer_ip:
                    details.setdefault('ip', peer_ip)
                    details.setdefault('ips', [])
                    if peer_ip not in details['ips']:
                        details['ips'].append(peer_ip)

                # å°è¯•è§£æ titleï¼ˆå³ä½¿ content-type ç¼ºå¤±ä¹Ÿå°è¯•ï¼‰
                title_found = False
                try:
                    content_bytes = await response.content.read(MAX_RESPONSE_SIZE)
                    details['sampled_bytes'] = len(content_bytes)
                    content = content_bytes.decode('utf-8', errors='ignore')
                    lower = content.lower()
                    if '<title>' in lower:
                        start = lower.find('<title>') + 7
                        end = lower.find('</title>', start)
                        if start > 6 and end > start:
                            details['title'] = content[start:end].strip()[:100]
                            title_found = True
                except Exception:
                    pass

                # å¦‚æœæœªæ‹¿åˆ°æ ‡é¢˜ï¼Œé¢å¤–åšä¸€æ¬¡æ—  Range çš„å…œåº• GETï¼ˆä»é™åˆ¶è¯»å–å¤§å°ï¼‰
                if not title_found and response.status < 400:
                    try:
                        async with session.get(
                            url=url,
                            allow_redirects=True,
                            timeout=ClientTimeout(total=4)
                        ) as resp2:
                            details.update({
                                'status_code': resp2.status,
                                'scheme': urlparse(str(resp2.url)).scheme,
                                'content_type': resp2.headers.get('content-type', ''),
                                'content_length': resp2.headers.get('content-length', ''),
                                'server': resp2.headers.get('server', details.get('server', '')),
                                'response_time': (dt.datetime.now() - start_time).total_seconds(),
                                'final_url': str(resp2.url),
                                'redirected': bool(resp2.history),
                            })
                            peer_ip2 = _extract_peer_ip(resp2)
                            if peer_ip2:
                                details.setdefault('ip', peer_ip2)
                                details.setdefault('ips', [])
                                if peer_ip2 not in details['ips']:
                                    details['ips'].append(peer_ip2)

                            content_bytes = await resp2.content.read(MAX_RESPONSE_SIZE)
                            details['sampled_bytes'] = len(content_bytes)
                            content = content_bytes.decode('utf-8', errors='ignore')
                            lower = content.lower()
                            if '<title>' in lower:
                                start = lower.find('<title>') + 7
                                end = lower.find('</title>', start)
                                if start > 6 and end > start:
                                    details['title'] = content[start:end].strip()[:100]
                                    title_found = True
                    except Exception:
                        pass
                if not title_found:
                    # å¸¦ä¸Šè°ƒè¯•ä¿¡æ¯ï¼Œæ–¹ä¾¿æ—¥å¿—æ’æŸ¥
                    details['title_debug'] = f"no <title> in first {details.get('sampled_bytes','?')} bytes; ct={details.get('content_type','')}; url={details.get('final_url','')}"
                    # ä»…åœ¨ debug çº§åˆ«è¾“å‡ºæˆªæ–­çš„æ­£æ–‡å†…å®¹
                    logger.debug(
                        "No <title> for {} {} ct={} sampled={} url={} body='{}'",
                        url,
                        response.status,
                        details.get('content_type'),
                        details.get('sampled_bytes'),
                        details.get('final_url'),
                        _safe_snippet(content if 'content' in locals() else ""),
                    )

                return True, details

            return False, details

    except Exception as e:
        details['error'] = f'get_error: {type(e).__name__}'
        return False, details


async def _enrich_with_get(
    session: aiohttp.ClientSession,
    subdomain: str,
    details: Dict,
) -> Dict:
    """
    å¯¹å·²ç¡®è®¤å­˜æ´»çš„å­åŸŸå†å‘èµ·ä¸€æ¬¡å—é™ GETï¼Œæå–çŠ¶æ€ç å’Œ titleã€‚
    é¿å…å¯¹æœªå­˜æ´»çš„ç›®æ ‡é‡å¤è¯·æ±‚ï¼Œå‡å°è´Ÿè½½ã€‚
    """
    if details.get('method', '').upper().startswith('GET'):
        return details

    detected_by = details.get('detected_by', details.get('method'))
    scheme = details.get('scheme') or 'https'
    ok, enriched = await _try_limited_get(
        session,
        f"{scheme}://{subdomain}",
        details.copy(),
        dt.datetime.now()
    )
    if ok:
        enriched['detected_by'] = detected_by
        return enriched

    details['detected_by'] = detected_by
    return details


def _is_valid_response(status_code: int) -> bool:
    """åˆ¤æ–­HTTPçŠ¶æ€ç æ˜¯å¦è¡¨ç¤ºæœ‰æ•ˆçš„HTTPæœåŠ¡"""
    if 200 <= status_code < 400:
        return True

    valid_4xx = {
        400, 401, 403, 404, 405, 406, 407, 408, 409, 410,
        411, 412, 413, 414, 415, 416, 417, 418, 421, 422,
        423, 424, 425, 426, 428, 429, 431, 451,
    }

    return status_code in valid_4xx


async def _update_run(
    session: AsyncSession,
    run: SubdomainRun,
    *,
    status: Optional[str] = None,
    log_line: Optional[str] = None,
    error: Optional[str] = None,
    finished: bool = False,
) -> None:
    if status:
        run.status = status
    if log_line:
        run.log_snippet = _append_log(run.log_snippet, log_line)
    if error:
        run.error_message = error
    if run.started_at is None and status == "running":
        run.started_at = dt.datetime.now(dt.timezone.utc)
    if finished:
        run.finished_at = dt.datetime.now(dt.timezone.utc)
    session.add(run)
    await session.commit()
    await session.refresh(run)


async def _ensure_wordlist(
    session: AsyncSession,
    wordlist_id: Optional[int],
    *,
    expected_type: str = DEFAULT_WORDLIST_TYPE,
) -> Optional[str]:
    if wordlist_id is None:
        stmt = (
            select(Wordlist)
            .where(Wordlist.is_default.is_(True), Wordlist.type == expected_type)
            .limit(1)
        )
        result = await session.exec(stmt)
        wordlist = result.first()
    else:
        stmt = select(Wordlist).where(
            Wordlist.id == wordlist_id, Wordlist.type == expected_type
        )
        result = await session.exec(stmt)
        wordlist = result.first()
    if wordlist is None:
        return None
    return wordlist.path


async def run_http_enumerator(
    session: AsyncSession, run_id: int, domain: str, wordlist_id: Optional[int]
) -> None:
    """
    é«˜æ•ˆHTTPå­åŸŸåæšä¸¾å™¨ä¸»å‡½æ•°
    """
    run = await session.get(SubdomainRun, run_id)
    if run is None:
        return
    if is_stopped(run_id):
        return

    wordlist_path = await _ensure_wordlist(session, wordlist_id)
    if not wordlist_path:
        await _update_run(
            session, run,
            status="failed",
            error="æœªæ‰¾åˆ°å¯ç”¨çš„å­—å…¸æ–‡ä»¶",
            finished=True
        )
        return

    await _update_run(session, run, status="running")

    try:
        ssl_context = _get_ssl_context()
        connector = TCPConnector(
            ssl=ssl_context,
            limit=MAX_CONCURRENT_REQUESTS,
            ttl_dns_cache=300,
            use_dns_cache=True,
        )
        client_timeout = ClientTimeout(
            total=REQUEST_TIMEOUT,
            connect=3,
            sock_read=2
        )

        # è¯»å–å­—å…¸
        with open(wordlist_path, 'r', encoding='utf-8', errors='ignore') as f:
            words = [line.strip() for line in f if line.strip() and not line.startswith('#')]

        set_progress(run_id, len(words), 0)

        await _update_run(
            session, run,
            log_line=f"ğŸš€ å¯åŠ¨HTTPæšä¸¾å™¨ï¼š{len(words)} ä¸ªå€™é€‰å­åŸŸå"
        )
        await _update_run(
            session, run,
            log_line=f"âš¡ é…ç½®ï¼šå¹¶å‘={MAX_CONCURRENT_REQUESTS}, è¶…æ—¶={REQUEST_TIMEOUT}s, DNSé¢„æ£€æŸ¥=å¯ç”¨"
        )
        await _update_run(
            session, run,
            log_line=f"ğŸ¯ ç­–ç•¥ï¼šDNSè§£æ â†’ å¹¶è¡Œ HEAD(HTTPS+HTTP) â†’ OPTIONS â†’ GET(å—é™)"
        )

        # åˆå§‹åŒ– DNS è§£æå™¨
        dns_resolver = None
        try:
            import aiodns
            dns_resolver = aiodns.DNSResolver(timeout=DNS_TIMEOUT, tries=DNS_RETRIES)
            await _update_run(
                session, run,
                log_line=f"ğŸ” DNSè§£æå™¨å·²å¯ç”¨ï¼Œå°†å…ˆè¿‡æ»¤ä¸å­˜åœ¨çš„åŸŸå"
            )
        except ImportError:
            await _update_run(
                session, run,
                log_line=f"âš ï¸  æœªå®‰è£… aiodnsï¼Œè·³è¿‡ DNS é¢„æ£€æŸ¥ (pip install aiodns)"
            )
            logger.info("aiodns not installed, skip DNS pre-check")

        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
        found_domains: Set[str] = set()

        # âœ… å…³é”®ä¿®å¤ï¼šå°†æ‰€æœ‰ä½¿ç”¨ session çš„ä»£ç æ”¾åœ¨ async with å—å†…
        async with aiohttp.ClientSession(
            connector=connector,
            timeout=client_timeout,
            headers={'User-Agent': USER_AGENT}
        ) as client_session:

            async def check_subdomain(word: str) -> Optional[Tuple[str, Dict]]:
                subdomain = f"{word}.{domain}"

                # ç¬¬ä¸€æ­¥ï¼šDNS é¢„æ£€æŸ¥
                dns_exists, ips = await _dns_resolve(subdomain, dns_resolver)
                
                if not dns_exists:
                    # DNS ä¸å­˜åœ¨ï¼Œç›´æ¥è·³è¿‡
                    return None

                # ç¬¬äºŒæ­¥ï¼šHTTP éªŒè¯
                async with semaphore:
                    is_valid, details = await _verify_subdomain_http(subdomain, client_session)

                    if is_valid and subdomain not in found_domains:
                        found_domains.add(subdomain)
                        # å·²ç¡®è®¤å­˜æ´»åå†è¿›è¡Œä¸€æ¬¡å—é™ GET è·å–æ ‡é¢˜/çŠ¶æ€ç ç­‰è¯¦æƒ…
                        details = await _enrich_with_get(client_session, subdomain, details)
                        
                        # å°† DNS ä¿¡æ¯æ·»åŠ åˆ° metadata
                        if ips:
                            details['ips'] = ips
                        # è‹¥ HTTP è¿æ¥æå–åˆ°äº†å¯¹ç«¯ IP ä¹Ÿé™„åŠ ä¸Š
                        if details.get('ip'):
                            details.setdefault('ips', [])
                            if details['ip'] not in details['ips']:
                                details['ips'].append(details['ip'])
                        
                        return subdomain, details

                    return None

            # æ‰¹é‡å¤„ç†
            batch_size = 200
            total_found = 0
            total_dns_filtered = 0

            for i in range(0, len(words), batch_size):
                if is_stopped(run_id):
                    clear_progress(run_id)
                    await _update_run(
                        session, run,
                        status="canceled",
                        finished=True,
                        log_line="â¹ ä»»åŠ¡å·²è¢«ç”¨æˆ·åœæ­¢"
                    )
                    return

                batch = words[i:i + batch_size]
                batch_num = i // batch_size + 1
                total_batches = (len(words) + batch_size - 1) // batch_size

                await _update_run(
                    session, run,
                    log_line=f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}: {len(batch)} ä¸ªå€™é€‰åŸŸå"
                )
                logger.info(
                    "Processing batch {}/{} (size={}) for domain={}",
                    batch_num,
                    total_batches,
                    len(batch),
                    domain,
                )

                # å¹¶å‘éªŒè¯å½“å‰æ‰¹æ¬¡
                tasks = [check_subdomain(word) for word in batch]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                # æ‰¹é‡å¤„ç†ç»“æœ
                batch_found = 0
                batch_subdomains: List[Subdomain] = []
                batch_logs: List[str] = []

                for result in batch_results:
                    if isinstance(result, Exception):
                        logger.warning("Check subdomain failed: {}", result)
                        continue

                    if result:
                        subdomain, details = result
                        batch_found += 1
                        total_found += 1

                        # æ ¼å¼åŒ–æ—¥å¿—
                        info = f"âœ… {subdomain}"
                        info += f" [{details['method']} {details['scheme']} {details['status_code']}]"
                        if details.get('response_time'):
                            info += f" ({details['response_time']:.2f}s)"
                        if details.get('ips'):
                            ips_str = ', '.join(details['ips'][:3])  # æœ€å¤šæ˜¾ç¤º3ä¸ªIP
                            if len(details['ips']) > 3:
                                ips_str += f" +{len(details['ips'])-3}..."
                            info += f" IP:[{ips_str}]"
                        if details.get('server'):
                            info += f" - {details['server'][:30]}"
                        if details.get('title'):
                            info += f" - {details['title'][:50]}"
                        elif details.get('title_debug'):
                            info += f" - æ— æ ‡é¢˜({details['title_debug'][:120]})"

                        batch_logs.append(info)

                        batch_subdomains.append(
                            Subdomain(
                                run_id=run.id,
                                host=subdomain,
                                source="http_enumerator",
                                metadata_json=json.dumps(details, ensure_ascii=False, separators=(',', ':'))
                            )
                        )

                # æ‰¹é‡å†™å…¥æ—¥å¿—å’Œæ•°æ®åº“
                if batch_logs:
                    await _update_run(session, run, log_line="\n".join(batch_logs))

                if batch_subdomains:
                    session.add_all(batch_subdomains)
                    try:
                        await session.commit()
                    except IntegrityError:
                        await session.rollback()
                        logger.opt(exception=True).warning("Integrity error while saving batch, rolled back")

                if batch_found > 0:
                    await _update_run(
                        session, run,
                        log_line=f"ğŸ“ˆ æœ¬æ‰¹æ¬¡å‘ç° {batch_found} ä¸ªå­åŸŸåï¼Œæ€»è®¡ {total_found}"
                    )

                increment_progress(run_id, len(batch))

        # å®Œæˆ
        clear_progress(run_id)
        
        summary = f"ğŸ‰ HTTPæšä¸¾å®Œæˆï¼æ€»è®¡å‘ç° {total_found} ä¸ªçœŸå®å¯è®¿é—®çš„å­åŸŸå"
        if dns_resolver:
            summary += f"\nğŸ” DNSé¢„æ£€æŸ¥å·²è¿‡æ»¤å¤§é‡æ— æ•ˆåŸŸå"
        
        await _update_run(
            session, run,
            status="succeeded",
            finished=True,
            log_line=summary
        )
        clear_stop(run_id)

    except Exception as e:
        clear_progress(run_id)
        clear_stop(run_id)
        await _update_run(
            session, run,
            status="failed",
            error=f"HTTPæšä¸¾å™¨é”™è¯¯: {str(e)}",
            finished=True
        )
